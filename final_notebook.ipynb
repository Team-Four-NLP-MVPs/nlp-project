{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee0c55bf-1e7a-47b7-b2d7-6b15780eb411",
   "metadata": {},
   "source": [
    "## Predicting Programming Languages\n",
    "### Natural Language Processing Among GitHub Repositories\n",
    "By: _AJ Martinez,        \n",
    "Ben Smith,        \n",
    "Nicholas Dougherty_          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c733d06-15ad-40cb-8d58-115a0267c4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# imports for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "from PIL import Image\n",
    "\n",
    "# import modules \n",
    "from prepare import * \n",
    "import acquire \n",
    "#import explore \n",
    "#import model \n",
    "\n",
    "# imports for NLP extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# imports for modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, recall_score, plot_confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a245d6a2-601f-4b27-8fc9-13ccc33e0669",
   "metadata": {},
   "source": [
    "***\n",
    "## Overview and Goals\n",
    "\n",
    "The goal of this project is to determine the main coding language of a project based on the contents of it's github Readme, using NLP methods. The data was acquired from various repositories on Github. In order to recreate this project you will need to access the json of the data we acquired. During the acquisition of the repo names, we filtered for the word customer, not for any particular reason other than something to filter for.\n",
    "\n",
    "A total of 193 Repos were obtained but after dropping nulls, readmes with Chinese characters, and slimming it down to the top most prevalent languages in our dataset we ended up with data from 106 different documents. The 4 languages that we filtered for were, Java, JavaScript, PHP, and Jupyter Notebook.\n",
    "\n",
    "## Findings\n",
    "\n",
    "We found that a () model using Lemmatized data performed the highest with an accuracy of () on the validate data set. With a final test accuracy of (). This outperformed our baseline accuracy of (). Our model was (list results and whatnot.)\n",
    "\n",
    "### With More Time\n",
    "\n",
    "We'd like to acquire more data to see if we can improve the results for distinguishing among (). Our sample size was fairly small during this project.\n",
    "*** \n",
    "## Acquisition and Preparation\n",
    "\n",
    "Data was obtained through functions that scraped repository collections on GitHub. First we manually explored GitHub using Chrome to inspect the HTML elements; the requests module obtained the HTML as a list of Universal Resource Locator (URL) endpoints, which were garnered from the trending portal and then appended to the origin. BeautifulSoup was essential in this regard. We scripted the process of requesting other pages, obtaining the README data form those as well through over a hundred repositories.       \n",
    "Here is a segment of the code used, the full code  can be viewed in the acquire.py script elsewhere in our repository. \n",
    "\n",
    "```\n",
    "# create an empty list to store endpoints\n",
    "    endpoints = []\n",
    "    # go to each url - trending repos daily, weekly, and monthly\n",
    "    for url in ['https://github.com/trending?since=daily&spoken_language_code=en',\n",
    "                'https://github.com/trending?since=weekly&spoken_language_code=en',\n",
    "                'https://github.com/trending?since=monthly&spoken_language_code=en']:\n",
    "        # get the response\n",
    "        response = get(url)\n",
    "        # create the beautiful soup object; It creates a parse tree from page source code\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # identify html objects containing each repository\n",
    "        for repo in soup.select('.Box-row'):\n",
    "            # pull out the url endpoint for that repo and append to the list\n",
    "            endpoints.append(repo\n",
    "                             .select_one('h1')\n",
    "                             .select_one('a')\n",
    "                             .attrs['href'])\n",
    "```\n",
    "The assimilated data was stored in a .json file, which was then used to obtain our Dataframe, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "803419a4-d9ae-4888-8d5a-c448faa948a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div id=4a638f28-d791-48be-a131-c61e4ddc1a71 style=\"display:none; background-color:#9D6CFF; color:white; width:200px; height:30px; padding-left:5px; border-radius:4px; flex-direction:row; justify-content:space-around; align-items:center;\" onmouseover=\"this.style.backgroundColor='#BA9BF8'\" onmouseout=\"this.style.backgroundColor='#9D6CFF'\" onclick=\"window.commands?.execute('create-mitosheet-from-dataframe-output');\">See Full Dataframe in Mito</div> <script> if (window.commands.hasCommand('create-mitosheet-from-dataframe-output')) document.getElementById('4a638f28-d791-48be-a131-c61e4ddc1a71').style.display = 'flex' </script> <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo</th>\n",
       "      <th>language</th>\n",
       "      <th>readme_contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>google/googletest</td>\n",
       "      <td>C++</td>\n",
       "      <td># GoogleTest\\n\\n### Announcements\\n\\n#### Live at Head\\n\\nGoogleTest now follows the\\n[Abseil Live at Head philosophy](https://abseil.io/about/philosophy#upgrade-support).\\nWe recommend\\n[updating to the latest commit in the `main` branch as often as possible](https://github.com/abseil/abseil-cpp/blob/master/FAQ.md#what-is-live-at-head-and-how-do-i-do-it).\\n\\n#### Documentation Updates\\n\\nOur documentation is now live on GitHub Pages at\\nhttps://google.github.io/googletest/. We recommend browsing the documentation on\\nGitHub Pages rather than directly in the repository.\\n\\n#### Release 1.11.0\\n\\n[Release 1.11.0](https://github.com/google/googletest/releases/tag/release-1.11.0)\\nis now available.\\n\\n#### Coming Soon\\n\\n*   We are planning to take a dependency on\\n    [Abseil](https://github.com/abseil/abseil-cpp).\\n*   More documentation improvements are planned.\\n\\n## Welcome to **GoogleTest**, Google's C++ test framework!\\n\\nThis repository is a merger of the formerly separate GoogleTest and GoogleMock\\nprojects. These were so closely related that it makes sense to maintain and\\nrelease them together.\\n\\n### Getting Started\\n\\nSee the [GoogleTest User's Guide](https://google.github.io/googletest/) for\\ndocumentation. We recommend starting with the\\n[GoogleTest Primer](https://google.github.io/googletest/primer.html).\\n\\nMore information about building GoogleTest can be found at\\n[googletest/README.md](googletest/README.md).\\n\\n## Features\\n\\n*   An [xUnit](https://en.wikipedia.org/wiki/XUnit) test framework.\\n*   Test discovery.\\n*   A rich set of assertions.\\n*   User-defined assertions.\\n*   Death tests.\\n*   Fatal and non-fatal failures.\\n*   Value-parameterized tests.\\n*   Type-parameterized tests.\\n*   Various options for running the tests.\\n*   XML test report generation.\\n\\n## Supported Platforms\\n\\nGoogleTest requires a codebase and compiler compliant with the C++11 standard or\\nnewer.\\n\\nThe GoogleTest code is officially supported on the following platforms.\\nOperating systems or tools not listed below are community-supported. For\\ncommunity-supported platforms, patches that do not complicate the code may be\\nconsidered.\\n\\nIf you notice any problems on your platform, please file an issue on the\\n[GoogleTest GitHub Issue Tracker](https://github.com/google/googletest/issues).\\nPull requests containing fixes are welcome!\\n\\n### Operating Systems\\n\\n*   Linux\\n*   macOS\\n*   Windows\\n\\n### Compilers\\n\\n*   gcc 5.0+\\n*   clang 5.0+\\n*   MSVC 2015+\\n\\n**macOS users:** Xcode 9.3+ provides clang 5.0+.\\n\\n### Build Systems\\n\\n*   [Bazel](https://bazel.build/)\\n*   [CMake](https://cmake.org/)\\n\\n**Note:** Bazel is the build system used by the team internally and in tests.\\nCMake is supported on a best-effort basis and by the community.\\n\\n## Who Is Using GoogleTest?\\n\\nIn addition to many internal projects at Google, GoogleTest is also used by the\\nfollowing notable projects:\\n\\n*   The [Chromium projects](http://www.chromium.org/) (behind the Chrome browser\\n    and Chrome OS).\\n*   The [LLVM](http://llvm.org/) compiler.\\n*   [Protocol Buffers](https://github.com/google/protobuf), Google's data\\n    interchange format.\\n*   The [OpenCV](http://opencv.org/) computer vision library.\\n\\n## Related Open Source Projects\\n\\n[GTest Runner](https://github.com/nholthaus/gtest-runner) is a Qt5 based\\nautomated test-runner and Graphical User Interface with powerful features for\\nWindows and Linux platforms.\\n\\n[GoogleTest UI](https://github.com/ospector/gtest-gbar) is a test runner that\\nruns your test binary, allows you to track its progress via a progress bar, and\\ndisplays a list of test failures. Clicking on one shows failure text. GoogleTest\\nUI is written in C#.\\n\\n[GTest TAP Listener](https://github.com/kinow/gtest-tap-listener) is an event\\nlistener for GoogleTest that implements the\\n[TAP protocol](https://en.wikipedia.org/wiki/Test_Anything_Protocol) for test\\nresult output. If your test runner understands TAP, you may find it useful.\\n\\n[gtest-parallel](https://github.com/google/gtest-parallel) is a test runner that\\nruns tests from your binary in parallel to provide significant speed-up.\\n\\n[GoogleTest Adapter](https://marketplace.visualstudio.com/items?itemName=DavidSchuldenfrei.gtest-adapter)\\nis a VS Code extension allowing to view GoogleTest in a tree view and run/debug\\nyour tests.\\n\\n[C++ TestMate](https://github.com/matepek/vscode-catch2-test-adapter) is a VS\\nCode extension allowing to view GoogleTest in a tree view and run/debug your\\ntests.\\n\\n[Cornichon](https://pypi.org/project/cornichon/) is a small Gherkin DSL parser\\nthat generates stub code for GoogleTest.\\n\\n## Contributing Changes\\n\\nPlease read\\n[`CONTRIBUTING.md`](https://github.com/google/googletest/blob/master/CONTRIBUTING.md)\\nfor details on how to contribute to this project.\\n\\nHappy testing!\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div>"
      ],
      "text/plain": [
       "                repo language  \\\n",
       "0  google/googletest      C++   \n",
       "\n",
       "                                     readme_contents  \n",
       "0  # GoogleTest\\n\\n### Announcements\\n\\n#### Live...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in data from the JSON created through acquire\n",
    "df = pd.read_json('data1.json')\n",
    "# View the content of the first row \n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6aa17ed-ec98-46bd-830e-8719f8e42317",
   "metadata": {},
   "source": [
    "From here, we break our data down into smaller component via parsing tools from the nltk packaged library. \n",
    "- All text was converted to lowercase for the sake of normalcy\n",
    "- Removed:\n",
    "    - accented, non-ASCII characters\n",
    "    - special characters\n",
    "    - stopwords\n",
    "- Words were stemmed and lemmatized as well. \n",
    "\n",
    "All of these processes were combined into a single function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d7d389b-f700-4f2d-9270-044975deb9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the dataframe and return text stemmed, lemmatized, cleaned, tokenized, et cetera\n",
    "df = prep_repos(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a511b443-26b3-49a2-a3fb-7b14da173cf5",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7213e313-968b-474f-81a7-f34e973d9349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e450eb3b-f203-49f6-aacd-298c5e44e4c0",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08db3b13-c698-4e52-bef1-e05c4832ddb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
